# gb_parsing
Методы сбора и обработки данных из сети Интернет

## Lesson 01
### Основы клиент-серверного взаимодействия. Парсинг API

1. Посмотреть документацию к API GitHub, разобраться как вывести список репозиториев для конкретного пользователя, сохранить JSON-вывод в файле *.json.

2. Изучить список открытых API (https://www.programmableweb.com/category/all/apis). Найти среди них любое, требующее авторизацию (любого типа). Выполнить запросы к нему, пройдя авторизацию. Ответ сервера записать в файл. 
В решении задания взят API вконтакте (https://vk.com/dev/first_guide) и сделан запрос на получение списка всех сообществ на которые подписан пользователь.


## Lesson 02
### Парсинг HTML. BeautifulSoup
#### Задание 1
Необходимо собрать информацию о вакансиях на вводимую должность (используем input или через аргументы) с сайтов superjob.ru и hh.ru. Приложение должно анализировать несколько страниц сайта (также вводим через input или аргументы). Получившийся список должен содержать в себе минимум:

- Наименование вакансии
- Предлагаемую зарплату (отдельно мин. и и отдельно макс.)
- Ссылку на саму вакансию
- Сайт откуда собрана вакансия

По желанию можно добавить ещё параметры вакансии (например, работодателя и расположение). Структура должна быть одинаковая для вакансий с обоих сайтов. Общий результат можно вывести с помощью dataFrame через pandas.


#### Задание 2
Источник https://geekbrains.ru/posts/
Необходимо обойти все записи в блоге и извлечь из них информацию следующих полей:

- url страницы материала
- Заголовок материала
- Первое изображение материала
- Дата публикации (в формате datetime)
- имя автора материала
- ссылка на страницу автора материала

Пример словаря:

{

"url": "str",

"title": "str",

"image": "str",

"writer_name": "str",

"writer_url": "str",

"pub_date": datetime object,


}

Полученые материалы сохранить в MongoDB. Предусмотреть метод извлечения данных из БД за период передаваемый в качестве параметров


## Lesson 03
### Системы управления базами данных MongoDB и SQLite в Python

1. Развернуть у себя на компьютере/виртуальной машине/хостинге MongoDB и реализовать функцию, записывающую собранные вакансии в созданную БД

2. Написать функцию, которая производит поиск и выводит на экран вакансии с заработной платой больше введенной суммы

3. Написать функцию, которая будет добавлять в вашу базу данных только новые вакансии с сайта


## Lesson 04
### Парсинг HTML. XPath

Написать приложение, которое собирает основные новости с сайтов mail.ru, lenta.ru, yandex-новости. Для парсинга использовать XPath. Структура данных должна содержать:

- название источника,
- наименование новости,
- ссылку на новость,
- дата публикации

Собрать все в базу данных (MongoDB)


## Lesson 05
### Scrapy

#### Задание 1

Доработать паука в имеющемся проекте, чтобы он формировал item по структуре:

- Наименование вакансии,
- Зарплата от,
- Зарплата до,
- Ссылку на саму вакансию,
- Сайт откуда собрана вакансия.

И складывал все записи в БД(любую)


#### Задание 2

Создать в имеющемся проекте второго паука по сбору вакансий с сайта superjob. 
Паук должен формировать item'ы по аналогичной структуре и складывать данные также в БД.

* Измерить скорость сбора вакансий в проекте и сравнить ее с проектом, выполненном с использованием BS+requests


## Lesson 06
### Scrapy. Парсинг фото и файлов
#### Задание 1

Взять авито Авто. Собирать с использованием ItemLoader следующие данные:

- название,
- все фото,
- параметры Авто

С использованием output_processor и input_processor реализовать очистку и преобразование данных. Значения цен должны быть в виде числового значения.


#### Задание 2

Дополнительно: Перевести всех пауков сбора данных о вакансиях на ItemLoader и привести к единой структуре.


## Lesson 07
### Selenium в Python
#### Задание 1

Написать программу, которая собирает входящие письма из своего или 
тестового почтового ящика и сложить данные о письмах в базу данных 
(от кого, дата отправки, тема письма, текст письма полный).

#### Задание 2

Написать программу, которая собирает «Хиты продаж» с сайта техники mvideo 
и складывает данные в БД. Магазины можно выбрать свои. 
Главный критерий выбора (если будете брать другой сайт): динамически загружаемые товары
Минимальный набор: наименование, ссылка и цена.


